---
title: k8s-nacos集群部署
date: 2022-03-30 01:22:08
tags:
  - k8s
  - nacos
categories:
description: 记录在k8s集群中部署nacos集群的操作步骤
---

 



拉取nacos部署包

`git clone https://github.com/nacos-group/nacos-k8s.git`

## 快速部署

```
cd nacos-k8s
chmod +x quick-startup.sh
./quick-startup.sh
```

缺点：未做持久化，存在数据丢失风险

## 高级部署

### 1.NFS

前提：已部署nfs服务（这里nfs server部署在192.168.1.201 共享目录/data/nfs）

#### 部署storageclass

**helm部署方式：**

```
helm repo add apphub https://apphub.aliyuncs.com
helm install  nfs-client apphub/nfs-client-provisioner --set nfs.server=192.168.1.201 --set nfs.path=/data/nfs
```





**手动部署方式：**

创建角色

`kubectl create -f deploy/nfs/rbac.yaml`

创建ServiceAccount并部署NFS-Client Provisioner

` vi deploy/nfs/deployment.yaml`

```
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "list", "watch", "create", "delete"]
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["get", "list", "watch", "update"]
- apiGroups: [""]
  resources: ["endpoints"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["events"]
  verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
- kind: ServiceAccount
  name: nfs-client-provisioner
  namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
rules:
- apiGroups: [""]
  resources: ["endpoints"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
subjects:
- kind: ServiceAccount
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io[root@cluster1 nfs]# cat deployment.yaml 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nfs-client-provisioner
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccount: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: quay.io/external_storage/nfs-client-provisioner:latest
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: fuseim.pri/ifs
            - name: NFS_SERVER
              value: 192.168.1.201
            - name: NFS_PATH
              value: /data/nfs
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.1.201
            path: /data/nfs
```

server为nfs服务地址，path为nfs共享的目录

```shell
kubectl create -f deploy/nfs/deployment.yaml
```

 创建 NFS StorageClass

```
kubectl create -f deploy/nfs/class.yaml
```

验证

`kubectl get pod -l app=nfs-client-provisioner`

**遇到的问题：**

由于我安装的是1.20版本的k8s，该版本禁用了selfLink，导致Provisioner在自动生成pv时一直报`unexpected error getting claim reference: selfLink was empty, can't make reference`，使pvc一直处于pending状态。

**解决办法：**

修改`/etc/kubernetes/manifests/kube-apiserver.yaml` 

```
spec:
  containers:
  - command:
    - kube-apiserver
    - --feature-gates=RemoveSelfLink=false  #不禁用selflink
    - --advertise-address=192.168.1.101
    - --allow-privileged=true
```

加上注释的这行，稍后apiserver会自动重启



### 2.部署数据库

```
vi deploy/mysql/mysql-nfs.yaml
```

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: mysql
  labels:
    name: mysql
spec:
  replicas: 1
  selector:
    name: mysql
  template:
    metadata:
      labels:
        name: mysql
    spec:
      containers:
      - name: mysql
        image: nacos/nacos-mysql:5.7
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mysql-data
          mountPath: /var/lib/mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "root"
        - name: MYSQL_DATABASE
          value: "nacos_devtest"
        - name: MYSQL_USER
          value: "nacos"
        - name: MYSQL_PASSWORD
          value: "nacos"
      volumes:
      - name: mysql-data
        nfs:
          server: 192.168.1.201
          path: /data/nfs/mysql
---
apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    name: mysql
spec:
  ports:
  - port: 3306
    targetPort: 3306
  selector:
    name: mysql
```

修改nfs的地址与路径

为了能从外网访问这里service设置为nodeport

```
apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    name: mysql
spec:
  type: NodePort
  ports:
  - port: 3306
    targetPort: 3306
    nodePort: 30006
  selector:
    name: mysql
```

```
kubectl create -f deploy/mysql/mysql-nfs.yaml
```

### 3.部署nacos

` vi depoly/nacos/nacos-pvc-nfs.yaml`

```
---
#cm里数据库配置根据实际情况修改
apiVersion: v1
kind: ConfigMap
metadata:
  name: nacos-cm
data:
  mysql.db.name: "nacos_devtest"
  mysql.port: "3306"
  mysql.user: "nacos"
  mysql.password: "nacos"
---
#statefuset中storageclass名根据部署情况修改
volumeClaimTemplates:
    - metadata:
        name: plugindir
        annotations:
          volume.beta.kubernetes.io/storage-class: "nfs-client" #与自己部署的storageclass名一致
      spec:
        accessModes: [ "ReadWriteMany" ]
        resources:
          requests:
            storage: 5Gi


```

`kubectl create -f nacos-k8s/deploy/nacos/nacos-pvc-nfs.yaml`

**配置ingress**

创建用于外网访问的ingress

`vi nacos-ingress.yaml`

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nacos-ingress
  namespace: default
  
spec:
  rules:
  - host: nacos.hiztc.com
    http:
      paths:
      - path: /
        pathType: ImplementationSpecific
        backend:
          service:
            name: nacos-headless
            port:
              number: 8848
```

`kubectl apply -f nacos-ingress.yaml`

```
[root@cluster1 nacos]# kubectl get svc -n ingress-nginx
NAME                                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             NodePort    10.10.234.71    <none>        80:30080/TCP,443:31268/TCP   5d4h
ingress-nginx-controller-admission   ClusterIP   10.10.127.130   <none>        443/TCP                      5d4h
```

通过ingress-controller暴露的nodePort访问，并配置好host：

```
123.xxx.xxx.xxx nacos.hiztc.com
```

访问`http://nacos.hiztc.com:30080/nacos`登陆nacos管理界面

