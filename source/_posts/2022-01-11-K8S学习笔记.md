---
title: K8S学习笔记
date: 2022-01-11 19:27:39
tags:
categories:
  - k8s
description: k8s学习记录
---

 

传统服务模型与容器化服务模型

![](container.png)

# k8s系统组件

![components-of-kubernetes](components-of-kubernetes.svg)

## 控制平面组件

控制平面的组件对集群做出全局决策(比如调度)，以及检测和响应集群事件（例如，当不满足部署的 `replicas` 字段时，启动新的 pod）。

**ApiServer**

ApiServer负责对外提供RESTful的 Kubernetes API。是 Kubernetes 控制平面接收指令的入口。任何对资源进行增删改查的操作都要交给APIServer处理后再提交给etcd

**etcd**

etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。

**scheduler**

负责调度pod到合适的Node上。调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范等。

**controller-manager**

用于在主节点上管理控制器的组件，每个资源一般都对应有一个控制器，这些控制器包括：

- 节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应。
- 副本控制器（Replication Controller）: 负责为系统中的每个副本控制器对象维护正确数量的 Pod。
- 端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod)。
- 服务帐户和令牌控制器（Service Account & Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌.

## Node组件

**kubelet**

一个在集群中每个[节点（node）](https://kubernetes.io/zh/docs/concepts/architecture/nodes/)上运行的代理。 它保证[容器（containers）](https://kubernetes.io/zh/docs/concepts/overview/what-is-kubernetes/#why-containers)都 运行在 [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) 中。

kubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 只管理由 Kubernetes 创建的容器。

**kube-proxy**

是集群中每个节点上运行的网络代理， 用于实现 [服务（Service）](https://kubernetes.io/zh/docs/concepts/services-networking/service/) 。kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。

**容器运行时(Container Runtime)**

Kubernetes 支持多个容器运行环境: [Docker](https://kubernetes.io/zh/docs/reference/kubectl/docker-cli-to-kubectl/)、 [containerd](https://containerd.io/docs/)、[CRI-O](https://cri-o.io/#what-is-cri-o) 以及任何实现 [Kubernetes CRI (容器运行环境接口)](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md)的容器环境。

# 核心概念

**master**

集群的控制节点，每个K8s集群里需要有一个Master节点来负责整个集群的管理和控制。基本上k8s的所有控制命令都发给它，它来负责整个具体的执行过程。Master节点通常占据一个独立的服务器（高可用部署建议3台服务器）。

Master节点运行着以下关键进程：

- API Server：集群控制的入口进程。
- Controller Manager：K8s里所有资源对象的自动化控制中心
- Scheduler:负责资源调度的进程。
- etcd:分布式持久化，所有资源对象的数据全部保存在etcd。

**node**

Node节点是K8s集群中的工作负载节点，当某个Node宕机，其上的工作负载会被Master自动转移到其他节点上。

Node节点运行着以下关键进程：

- kubelet: 负责Pod对应容器的创建、启停等任务。
- kube-proxy: 实现k8s service的通信和负载均衡机制的重要组件。

**pod**

- k8s集群中最小的部署单元
- 一组容器的集合
- 一个pod具有独立网络ip，
- 生命周期短暂，用完即焚 永久销毁

**controller**

​		当定义了一个controller并提交到了K8s集群以后，Master上的Controller Manager组件就得到通知，定时巡检系统中目前存活的目标Pod,并且确保目标Pod实例的数量刚好等于此controller的期望值。如果有过多的副本在运行，系统就会停掉一些Pod,否则系统就会再自动创建一些Pod。controller有多种类型：deployment、statefulSet、DaemonSet、Job、CronJob等

- 确保预期的pod副本数量
- 无状态应用部署，一般为deployment，相当于ReplicationController 的升级版
- 有状态应用部署，一般用statefulSet

**service**

​		service定义一组pod的访问规则，可以看成一组Pod的对外访问接口。集群中的pod可以无障碍的访问service。service常见有以下三种类型，应对不同场景的服务开放

- ClusterIp模式：只对集群内部提供服务

- Nodeport模式：在每个node上开放端口，对外提供服务
- LoadBalance模式：负载均衡，一般配合云服务平台提供的负载均衡器使用



# 命令行工具kubectl

`kubectl --help`

```shell
Basic Commands (Beginner):
  create         Create a resource from a file or from stdin.
  expose         使用 replication controller, service, deployment 或者 pod 并暴露它作为一个 新的
Kubernetes Service
  run            在集群中运行一个指定的镜像
  set            为 objects 设置一个指定的特征
 
Basic Commands (Intermediate):
  explain        查看资源的文档
  get            显示一个或更多 resources
  edit           在服务器上编辑一个资源
  delete         Delete resources by filenames, stdin, resources and names, or by resources and label selector
 
Deploy Commands:
  rollout        Manage the rollout of a resource
  scale          为 Deployment, ReplicaSet, Replication Controller 或者 Job 设置一个新的副本数量
  autoscale      自动调整一个 Deployment, ReplicaSet, 或者 ReplicationController 的副本数量
 
Cluster Management Commands:
  certificate    修改 certificate 资源.
  cluster-info   显示集群信息
  top            Display Resource (CPU/Memory/Storage) usage.
  cordon         标记 node 为 unschedulable
  uncordon       标记 node 为 schedulable
  drain          Drain node in preparation for maintenance
  taint          更新一个或者多个 node 上的 taints
 
Troubleshooting and Debugging Commands:
  describe       显示一个指定 resource 或者 group 的 resources 详情
  logs           输出容器在 pod 中的日志
  attach         Attach 到一个运行中的 container
  exec           在一个 container 中执行一个命令
  port-forward   Forward one or more local ports to a pod
  proxy          运行一个 proxy 到 Kubernetes API server
  cp             复制 files 和 directories 到 containers 和从容器中复制 files 和 directories.
  auth           Inspect authorization
 
Advanced Commands:
  apply          通过文件名或标准输入流(stdin)对资源进行配置
  patch          使用 strategic merge patch 更新一个资源的 field(s)
  replace        通过 filename 或者 stdin替换一个资源
  wait           Experimental: Wait for one condition on one or many resources
  convert        在不同的 API versions 转换配置文件
 
Settings Commands:
  label          更新在这个资源上的 labels
  annotate       更新一个资源的注解
  completion     Output shell completion code for the specified shell (bash or zsh)
 
Other Commands:
  alpha          Commands for features in alpha
  api-resources  Print the supported API resources on the server
  api-versions   Print the supported API versions on the server, in the form of "group/version"
  config         修改 kubeconfig 文件
  plugin         Runs a command-line plugin
  version        输出 client 和 server 的版本信息

```

# 资源编排yaml

每一个k8s对象都可用yaml文件进行描述，kubelet依据yaml内容创建对象

示例

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2 # tells deployment to run 2 pods matching the template
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
```



字段说明

| 字段       | 含义       |
| ---------- | ---------- |
| apiVersion | API版本    |
| kind       | 资源类型   |
| metadata   | 资源元数据 |
| spec       | 资源规格   |
| replicas   | 副本数量   |
| selector   | 标签选择器 |
| template   | pod模板    |
| metadata   | pod元数据  |
| spec       | pod规格    |
| containers | 容器配置   |

快速生成yaml文件

- 方式一：kubectl create 生成

  ```shell
  kubectl create deployment web --image=nginx -o yaml --dry-run >my1.yaml
  ```

- 方式二：kubectl get 导出

  ```shell
  kubectl get deploy nginx -o yaml > my2.yaml
  ```


集群中的每一个对象都有一个 `name` 来标识在同一 `namespace` 下同类资源中的唯一性。

每个k8s对象都有一个 `uid` 来标识在整个集群中的唯一性

对于用户提供的非唯一性的属性，Kubernetes 提供了 [标签（Labels）](https://kubernetes.io/zh/docs/concepts/working-with-objects/labels)和 [注解（Annotation）](https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/annotations/)机制。



# 常见资源类型

## Pod

### 基本概念

- k8s中最小的部署单元
- 包含多个容器（一组容器的集合）
- 一个pod中容器共享网络命名空间
- pod是短暂的
- 每个pod有一个特殊的被称为“根容器”的Pause容器。Pause容器对应的镜像属于k8s平台的一部分，除了Pause容器，每个Pod还包含一个或多个紧密相关的用户业务容器

### pod存在意义

- 创建容器使用docker，一个容器运行一个程序，单进程设计
- pod是多进程设计，每个pod多个容器，一个容器运行一个程序
- pod存在是为了亲密性应用（应用之间进行交互、网络调用）

### pod实现机制

- 共享网络

  通过Pause容器，将其他业务容器都加入到Pause容器，而让所有业务容器处于同一个host，实现网络共享

- 共享存储

  使用数据卷Volumn进行持久化存储

### 镜像拉取策略

```yaml
...
spec:
  containers:
    - name: nginx
      image: nginx:1.14
      imagePullPolicy: Always
...
```

- IfNotPresent：默认值，镜像在宿主机上不存在时才拉取
- Always：每次创建Pod都会重新拉取一次镜像
- Never：pod永远不会主动拉取这个镜像

### 资源限制

```yaml
...
spec:
  containers:
    - name: db
      image: mysql
      env: 
      - name: MYSQL_ROOT_PASSWORD
        value: "password"
      resources:
        requests:
          memory: "64Mi"
          cpu: "250m"
        limits:
          memory: "128Mi"
          cpu: "500m"
...
```

### Pod重启机制

```yaml
...
spec:
  containers:
    - name: busybox
      image: busybox:1.28.4
    restartPolicy: Always

...
```

- Always：当容器终止退出后，总是重启容器，默认策略
- OnFailure：当容器异常退出（退出状态码非0）时，才重启容器
- Never：当容器终止退出，从不重启容器

### Pod健康检查

```
...
spec:
  containers:
    - name: liveness
      image: busybox
      livenessProbe:
        exec:
          command:
          - cat
          - /tmp/healthy
        initialDelaySecond: 5
        periodSecond: 5
...
```



- livenessProbe(存活检查)

  如果检测失败，杀死容器，根据Pod的restartPolicy来操作

- readinessProbe(就绪检查)

  如果检查失败，将Pod从service endpoints中剔除

三种检查方式

1. httpGet ：发送请求返回200-400为成功
2. exec ： 执行Shell命令返回状态码0为成功
3. tcpSocket：发起TCP Socket建立连接成功

### Pod调度策略

1、create pod 时调用 apiserver 将pod信息存入 etcd
2、scheduler 通过 apiserver 发现有新pod需创建，再通过apiserver从etcd中获取pod信息，根据调度算法将pod调度到某个node节点
3、kubelet 通过apiserver读取etcd拿到分配给当前node的pod信息，docker创建容器

### 影响调度的因素

- 资源限制

  ```yaml
  requests:
    memory: "64Mi"
    cpu: "250m"
  ```

  根据request找到资源足够的node节点进行调度

- 标签选择器(可以用于部署开发、测试、生产环境)

  ```yaml
  spec:
    nodeSelector:
      env_role: dev
  ```

  根据node标签指定调度位置

  通过命令`kubectl label node cluster2 env_role=dev` 给node节点标记

  `kubectl get nodes --show-labels`查看标签

- 节点亲和性

  ```yaml
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            operator: In
            values:
            - dev
            - test
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          preference:
            matchExpression:
            - key: group
              operator: In
              values:
              - otherprod
  ```

  与nodeSelector差不多，根据标签决定调度到哪台node

  - 硬亲和性：约束条件必须满足
  - 软亲和性：尝试满足，但不保证

  支持操作符：In NotIn Exists Gt Lt DoesNotExists

- 污点

  nodSelector和nodeAffinity是Pod的属性，pod调度到某些节点上时实现

  Taint污点：节点不做普通分配，是节点的属性

  场景

  - 专用节点
  - 配置特定硬件节点
  - 基于Traint驱逐

  查看节点污点情况

  ```shell
  kubectl describe node cluster1 | grep Taint
  ```

  污点值有三个：

  - NoSchedule:一定不被调度
  - PreferNoSchedule:尽量不被调度
  - NoExecute：不会调度,并且还会驱逐Node已有Pod

  为节点添加污点：

  ```shell
  kubectl taint node [node] key=value:三个污点值
  ```

  示例

    ```shell
  #创建5个Nginx的pod
  kubectl create deployment web --image=nginx
  kubectl scale deployment web --replicas=5
  #此时可看到5个pod分布在各个节点
  kubectl get pods -o wide
  #删除pod
  kubectl delete deployment web
  #打污点
  kubectl taint node cluster2 env=yes:NoSchedule
  #查看污点
  kubectl describe node cluster2 | grep Taint
  #Taints:             env=yes:NoSchedule
  #再次创建5个pod
  kubectl create deployment web --image=nginx
  kubectl scale deployment web --replicas=5
  #此时可看到打上污点的node不再被分配pod
  kubectl get pods -o wide
  #删除污点
  kubectl taint node cluster2 env:NoSchedule-
    ```

- 污点容忍

  ```yaml
  spec:
    tolerations:  #容忍污点
    - key: "key"
      operator: "Equal"
      value: "value"
      effect: "NoSchedule"
  ```

  

## Controller(deployment)

### 基本概念

- Controller是在集群上管理和运行容器的对象
- Pod通过Controller实现应用的运维，如伸缩、滚动升级等
- Pod和Controller之间通过标签建立关系label-selector

### deployment控制器的应用

- 部署无状态应用
- 管理Pod和ReplicaSet
- 部署、滚动升级等

场景：web服务、微服务

### 使用deployment部署应用(yaml)

导出yaml文件

```
kubectl create deployment web --image=nginx -o yaml --dry-run >web.yaml
```

修改文件

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: web
  name: web
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: web
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}
```

使用yaml部署应用

```shell
kubectl apply -f web.yaml
```

对外进行发布（暴露对外端口）

```shell
#

kubectl expose deployment web --port=80 --type=NodePort --target-port=80 --name=web1 -o yaml>web1.yaml
```

```yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2020-12-25T03:58:07Z"
  labels:
    app: web
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:labels:
          .: {}
          f:app: {}
      f:spec:
        f:externalTrafficPolicy: {}
        f:ports:
          .: {}
          k:{"port":80,"protocol":"TCP"}:
            .: {}
            f:port: {}
            f:protocol: {}
            f:targetPort: {}
        f:selector:
          .: {}
          f:app: {}
        f:sessionAffinity: {}
        f:type: {}
    manager: kubectl-expose
    operation: Update
    time: "2020-12-25T03:58:06Z"
  name: web1
  namespace: default
  resourceVersion: "112853"
  uid: db90ad2d-8538-4844-9e2b-f01e5bff8b84
spec:
  clusterIP: 10.10.106.70
  clusterIPs:
  - 10.10.106.70
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 31014
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: web
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
```

### 应用升级回滚和弹性伸缩

升级

```shell
kubectl set image deployment web nginx=nginx:1.15
```

查看升级状态

```shell
kubectl rollout status deployment web
```

查看升级历史

```shell
kubectl rollout history deployment web
```

```
deployment.apps/web 
REVISION  CHANGE-CAUSE
1         <none>
2         <none>
```

回滚

```shell
kubectl rollout undo deployment web  #回滚到上一个版本
kubectl rollout undo deployment web --to-revision=1 #回滚到指定编号的版本
```

弹性伸缩

将pod副本的值伸缩至10个

```
kubectl scale deployment web --replicas=10
```

## Service

### service意义

- 防止Pod失联（服务发现），类似服务注册中心
- 定义pod访问策略（负载均衡）

### Pod与Service关系

- 通过service实现pod的负载均衡
- 根据label和selector建立关联关系

### 常用service类型

- ClusterIP  集群内部使用
- NodePort  对外访问应用
- LoadBalancer   对外访问应用，负载均衡

## Controller(StatefulSet、DaemonSet、Job、cronJob)

### 无状态应用：

- 认为所有Pod都是一样的
- 启动顺序无要求
- 不用考虑在哪个node运行
- 可随意伸缩扩展

### 有状态应用：

- 以上无状态应用的所有因素都需要考虑到

- 每个pod都是独立的，保证pod的启动顺序和唯一性
  - 有唯一的网络标识、持久化存储
  - 有序，比如mysql主从

### 部署有状态应用StatefulSet

`vi sts.yaml`

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx-statefulset
  namespace: default
spec:
  serviceName: nginx
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
```

`kubectl apply -f sts.yaml`

查看pod

```
[root@cluster1 ~]# kubectl get pod -o wide
NAME                  READY   STATUS    RESTARTS   AGE   IP               NODE       NOMINATED NODE   READINESS GATES
nginx-statefulset-0   1/1     Running   0          84s   10.122.61.9      cluster2   <none>           <none>
nginx-statefulset-1   1/1     Running   0          71s   10.122.146.205   cluster3   <none>           <none>
nginx-statefulset-2   1/1     Running   0          45s   10.122.146.206   cluster3   <none>           <none>
```

查看service

```
[root@cluster1 ~]# kubectl get svc -o wide
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
kubernetes   ClusterIP   10.10.0.1    <none>        443/TCP   28h     <none>
nginx        ClusterIP   None         <none>        80/TCP    5m28s   app=nginx
```

与deployment区别：

有唯一标识，每个pod有唯一主机名，唯一域名格式=主机名.service名.名称空间.svc.cluster.local

` nginx-statefulset-0.nginx.default.svc.cluster.local`

### 部署守护进程DaemonSet

`vi ds.yaml`

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ds-test
  labels:
    app: filebeat
spec:
  selector:
    matchLabels:
      app: filebeat
  template:
    metadata:
      labels:
        app: filebeat
    spec:
      containers:
      - name: logs
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - name: varlog
          mountPath: /tmp/log
      volumes:
      - name: varlog
        hostPath:
          path: /var/log

```

启动`kubectl apply -f ds.yaml`

进入一个pod中

`kubectl exec -it ds-test-5dszw bash `

```shell
root@ds-test-5dszw:/# cd /tmp/log/
root@ds-test-5dszw:/tmp/log# ls
audit	  boot.log-20201223  calico  cloud-init.log  cron   dmesg.old  grubby_prune_debug  maillog   pods     rhsm    spooler	tuned  yum.log
boot.log  btmp		     chrony  containers      dmesg  glusterfs  lastlog		   messages  qemu-ga  secure  tallylog	wtmp
root@ds-test-5dszw:/tmp/log# exit
exit
[root@cluster1 ~]#
```

### Job一次性任务

`vi job.yaml`

```yaml
apiVersion: batch/v1
kind: Job
metadata: 
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl","-Mbignum=bpi","-wle","print bpi(2000)"]
      restartPolicy: Never
  backoffLimit: 4

```

查看任务

`kubectl get jobs`

查看pod打印log

`kubectl logs pi-5qwh2 `

删除

`kubectl delete -f job.yaml`

### CronJob定时任务

`vi cron.yaml`

```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata: 
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from k8s
          restartPolicy: OnFailure
```

## 配置管理(Secret)

作用：加密数据存在etcd里，让Pod容器以挂载Volume方式进行访问，场景：凭证



创建secret加密数据

`vi secret.yaml`

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: cm9vdAo=
  password: aGlzZW5zZQo=
```

```shell
kubectl apply -f secret.yaml
kubectl get secret
```

**以变量形式挂载到pod容器**

`vi secret-val.yaml`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: nginx
    image: nginx
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: username
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: password
```

`kubectl apply -f secret-val.yaml`

进入容器查看

```
[root@cluster1 ~]# kubectl exec -it mypod bash
root@mypod:/# echo $SECRET_USERNAME
root
root@mypod:/# echo $SECRET_PASSWORD
hisense
root@mypod:/# 

```

**以数据卷(Volume)形式挂载到容器**

`vi secret-vol.yaml`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod2
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: mysecret
```

`kubectl apply -f secret-vol.yaml`

进入容器查看

```
[root@cluster1 ~]# kubectl exec -it mypod bash
root@mypod2:/# ls /etc/foo/
password  username
root@mypod2:/# cd /etc/foo/
root@mypod2:/etc/foo# cat password 
hisense
root@mypod2:/etc/foo# cat username 
root
```

## 配置管理(ConfigMap)

作用：存储非加密的数据到etcd，让pod以变量或volume挂载到容器，场景：配置文件

以redis为例：

**创建配置文件**

`vi redis.properties`

```properties
redis.host=127.0.0.1
redis.port=6379
redis.password=123456
```

**创建configmap**

```
kubectl create configmap redis-config --from-file=redis.properties
```

查看

```
[root@cluster1 ~]# kubectl get cm
NAME               DATA   AGE
kube-root-ca.crt   1      4d22h
redis-config       1      48s
[root@cluster1 ~]# kubectl describe cm redis-config
Name:         redis-config
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
redis.properties:
----
redis.host=127.0.0.1
redis.port=6379
redis.password=123456

Events:  <none>
```

**以volume挂载到容器**

`vi cm.yaml`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod3
spec:
  containers:
  - name: busybox
    image: busybox
    command: ["/bin/sh","-c","cat /etc/config/redis.properties"]
    volumeMounts:
    - name: config-volume
      mountPath: "/etc/config"
  volumes:
  - name: config-volume
    configMap:
      name: redis-config
  restartPolicy: Never
```

```shell
[root@cluster1 ~]# kubectl get cm
NAME               DATA   AGE
kube-root-ca.crt   1      4d22h
redis-config       1      2m46s
[root@cluster1 ~]# vi cm.yaml
[root@cluster1 ~]# kubectl apply -f cm.yaml 
pod/mypod3 created
[root@cluster1 ~]# kubectl get pod
NAME                  READY   STATUS      RESTARTS   AGE
mypod3                0/1     Completed   0          35s
[root@cluster1 ~]# kubectl logs mypod3
redis.host=127.0.0.1
redis.port=6379
redis.password=123456
```

**以变量形式挂载**

`vi myconfig.yaml`

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: myconfig
data:
  special.level: info
  special.type: hello
```

`kubectl apply -f myconfig.yaml`

创建pod

`vi config-val.yaml`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod4
spec:
  containers:
  - name: busybox
    image: busybox
    command: ["/bin/sh","-c","echo $(LEVEL) $(TYPE)"]
    env:
      - name: LEVEL
        valueFrom:
          configMapKeyRef:
            name: myconfig
            key: special.level
      - name: TYPE
        valueFrom:
          configMapKeyRef:
            name: myconfig
            key: special.type
  restartPolicy: Never
```

`kubectl apply -f config-var.yaml`

查看日志

```
[root@cluster1 ~]# kubectl logs mypod4
info hello
```

# k8s集群安全机制

## 概述

- 访问k8s集群时，需要经过三步：
  - 认证
    - 传输安全：对外不暴露8080端口，只能内部访问，对外使用端口6443
    - 认证：客户端认证常用方式：①https证书认证，基于ca证书 ②http token认证，通过token识别用户③http基本认证，用户名+密码认证
  - 授权（鉴权）
    - 基于RBAC进行鉴权操作，基于角色的访问控制
  - 准入控制
    - 准入控制器的列表，如列表有请求的内容，通过，无则拒绝
- 访问时都需要经过apiserver，apiserver做统一协调，访问过程中需要证书、token、或用户名密码，如访问pod还需要serviceAccount

## RBAC 基于角色的访问控制

- 角色：
  - role：特定命名空间访问权限
  - clusterRole：所有命名空间访问权限
- 角色绑定
  - roleBinding：角色绑定到主体
  - clusterRoleBinding：集群角色绑定到主体
- 主体
  - user：用户
  - group：用户组
  - serviceAccount：服务账号

**步骤**

1. 创建命名空间

   ```shell
   kubectl create ns roledemo
   kubectl get ns
   ```

2. 在该命名空间创建pod

   ```
   kubectl run nginx --image=nginx -n roledemo
   #查看pod
   kubectl get pod -n roledemo
   ```

3. 创建角色

   `vi rbac-role.yaml`

   ```yaml
   kind: Role
   apiVersion: rbac.authorization.k8s.io/v1
   metadata:
     namespace: roledemo
     name: pod-reader
   rules:
   - apiGroups: [""] # "" indicates the core API group
     resources: ["pods"]
     verbs: ["get","watch","list"]
   ```

   `kubectl apply -f rbac-role.yaml`

   查看

   `kubectl get role -n roledemo`

4. 创建角色绑定

   `vi rbac-rolebinding.yaml`

   ```yaml
   kind: RoleBinding
   apiVersion: rbac.authorization.k8s.io/v1
   metadata:
     namespace: roledemo
     name: read-pods
   subjects: 
   - kind: User
     name: lhb #Name is case sensitive
     apiGroup: rbac.authorization.k8s.io
   roleRef:
     kind: Role #this must be Role or ClusterRole
     name: pod-reader
    apiGroup: rbac.authorization.k8s.io
   ```

   `kubectl apply -f rbac-rolebinding.yaml`

   查看

   ```
   [root@cluster1 k8s-learn]# kubectl get role,rolebinding -n roledemo
   NAME                                        CREATED AT
   role.rbac.authorization.k8s.io/pod-reader   2020-12-29T09:56:37Z
   
   NAME                                              ROLE              AGE
   rolebinding.rbac.authorization.k8s.io/read-pods   Role/pod-reader   50s
   ```

5. 证书

# Ingress

Ingress和pod之间通过service关联，Ingress作为统一入口，由service关联一组pod

1. 部署ingress controller
2. 创建ingress规则

## 安装ingress controller

参考[官方说明](https://kubernetes.github.io/ingress-nginx/deploy/#using-helm)  

**安装方式一：**

下载安装文件

```
wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.43.0/deploy/static/provider/baremetal/deploy.yaml
```

由于众所周知的原因，在国内k8s.gcr.io仓库里的镜像是拉取不到的，因此需要手动在机器上拉取阿里的镜像并修改tag

先将下载下来的deploy.yaml去掉镜像`k8s.gcr.io/ingress-nginx/controller:v0.43.0@sha256:9bba603b99bf25f6d117cf1235b6598c16033ad027b143c90fa5b3cc583c5713`后的校验码，因为当k8s发现本地的镜像校验码不一致时还是会去k8s.gcr.io中拉取，修改后如下

```
spec:
      dnsPolicy: ClusterFirst
      containers:
        - name: controller
          image: k8s.gcr.io/ingress-nginx/controller:v0.43.0
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                  - /wait-shutdown
```



拉取nginx-ingress-controller镜像

`docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:v0.43.0`

重新打tag

`docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:v0.43.0 k8s.gcr.io/ingress-nginx/controller:v0.43.0`

安装

`kubectl apply -f deploy.yaml`

验证

```
kubectl get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx --watch
```

看到ingress-nginx-controller状态为running表示安装成功

**安装方式二(helm)：**

```shell
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update

helm install my-ingress-nginx ingress-nginx/ingress-nginx
```

```
NAME: my-ingress-nginx
LAST DEPLOYED: Fri Jan  8 13:37:21 2021
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The ingress-nginx controller has been installed.
It may take a few minutes for the LoadBalancer IP to be available.
You can watch the status by running 'kubectl --namespace default get services -o wide -w my-ingress-nginx-controller'

An example Ingress that makes use of the controller:

  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  metadata:
    annotations:
      kubernetes.io/ingress.class: nginx
    name: example
    namespace: foo
  spec:
    rules:
      - host: www.example.com
        http:
          paths:
            - backend:
                serviceName: exampleService
                servicePort: 80
              path: /
    # This section is only required if TLS is to be enabled for the Ingress
    tls:
        - hosts:
            - www.example.com
          secretName: example-tls

If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:

  apiVersion: v1
  kind: Secret
  metadata:
    name: example-tls
    namespace: foo
  data:
    tls.crt: <base64 encoded cert>
    tls.key: <base64 encoded key>
  type: kubernetes.io/tls
```

helm安装方式也需要解决镜像拉取不到的问题，思路与方式一类似

## 配置ingress

步骤

1. 创建nginx pod并暴露端口

   `kubectl create deployment web --image=nginx`

   `kubectl expose deployment web --port=80 --target-port=80 --type=NodePort`

2. 添加ingress

   ```yaml
   apiVersion: networking.k8s.io/v1beta1
   kind: Ingress
   metadata:
     name: example-ingress
   spec:
     rules:
     - host: example.ingredemo.com
       http:
         paths:
         - path: /
           backend:
             serviceName: web
             servicePort: 80
   
   ```

    `kubectl apply -f ingress.yaml`

   查看ingress controller

   `kubectl get pods -n ingress-nginx -o wide`

   查看ingress

   `kubectl get ing`

3. 访问

   添加映射 10.xx.xx.xx  example.ingredemo.com

# Helm

优势：使用helm可以将多个yaml作为一个整体管理；实现yaml高效复用；使用heml实现应用级别的版本管理

Helm是一个Kubernetes的包管理工具，就像Linux下的包管理器，如yum/apt等，可以很方便的将之前打包好的yaml文件部署到kubernetes上。

Helm有三个重要概念

- helm：一个命令行客户端工具，主要用于Kubernetes应用chart的创建、打包、发布和管理
- Chart：应用描述，一系列用于描述k8s资源相关文件的集合
- Release：基于Chart的部署实体，一个chart被Helm运行后将会生成对应的release，将在K8S中创建出真实的运行资源对象。也就是应用级别的版本管理
- Repository：用于发布和存储Chart的仓库 

## helm安装

下载helm压缩文件，解压，将helm文件拷贝到/usr/bin

## helm配置仓库

```shell
# 配置源
helm repo add azure http://mirror.azure.cn/kubernetes/charts
helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
#更新
helm repo update
#查看
helm repo list
#删除
helm repo remove aliyun
```

## 使用helm快速部署应用

1. 搜索需要安装的应用(weave)

   ```
   [root@cluster1 k8s-learn]# helm search repo weave
   NAME              	CHART VERSION	APP VERSION	DESCRIPTION                                       
   aliyun/weave-cloud	0.1.2        	           	Weave Cloud is a add-on to Kubernetes which pro...
   aliyun/weave-scope	0.9.2        	1.6.5      	A Helm chart for the Weave Scope cluster visual...
   azure/weave-cloud 	0.3.9        	1.4.0      	DEPRECATED - Weave Cloud is a add-on to Kuberne...
   azure/weave-scope 	1.1.12       	1.12.0     	DEPRECATED - A Helm chart for the Weave Scope c...
   
   ```

2. 安装应用

   ```shell
   # helm install 安装后的名称 需要安装的名称
   helm install ws aliyun/weave-scope
   
   #查看
   helm status ws
   kubectl get pod
   kubectl get svc
   ```

   修改service，改为NodePort方式 对外暴露端口

   ```shell
   #将spec.type由ClusterIP改为NodePort
   kubectl edit svc ws-weave-scope
   
   ```

   

## 自定义chart

1. 创建chart

   ```
   [root@cluster1 k8s-learn]# helm create mychart
   Creating mychart
   [root@cluster1 k8s-learn]# cd mychart/
   [root@cluster1 mychart]# ll -a
   total 16
   drwxr-xr-x 4 root root   93 Dec 30 17:18 .
   drwxr-xr-x 4 root root 4096 Dec 30 17:18 ..
   drwxr-xr-x 2 root root    6 Dec 30 17:18 charts
   -rw-r--r-- 1 root root 1098 Dec 30 17:18 Chart.yaml
   -rw-r--r-- 1 root root  349 Dec 30 17:18 .helmignore
   drwxr-xr-x 3 root root  162 Dec 30 17:18 templates
   -rw-r--r-- 1 root root 1800 Dec 30 17:18 values.yaml
   
   ```

   - Chart.yml：当前chart属性配置信息
   - templates：编写的yaml文件放在该目录
   - values.yaml：存放全局变量

2. templates中创建文件

   ```shell
   # 导出deployment.yaml
   kubectl create deployment web1 --image=nginx -o yaml > deployment.yaml
   # 导出service.yaml 【需要创建 deployment，不然会报错】
   kubectl expose deployment web1 --port=80 --target-port=80 --type=NodePort --dry-run -o yaml > service.yaml
   # 把刚创建的web1 pod删除
   kubectl delete deployment web1
   
   ```

   

3. 安装mychart

   ```
   helm install web1 mychart/
   ```

4. 应用升级

   ```
   helm upgrade web1 mychart/
   ```

## 实现yaml高效复用

通过传递参数，动态渲染模板，yaml内容动态从传入参数生成

1. 在values.yaml定义变量和值
2. 在具体yaml中获取变量值
3. yaml文件中大致有几个位置不同
   - image
   - tag
   - label
   - port
   - replicas

**实现步骤：**

1. values.yaml定义变量

   `vi values.yaml`

   ```
   replicas: 1
   image: nginx
   tag: 1.16
   label: nginx
   port: 80
   ```

2. templates的yaml中使用变量

   变量表达式：`{{ .Values.变量名称 }}`

   `vi deployment.yaml`

   ```
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     creationTimestamp: null
     labels:
       app: web1
     name: {{.Release.Name}}-deploy
   spec:
     replicas: 1
     selector:
       matchLabels:
         app: {{.Values.label}}
     strategy: {}
     template:
       metadata:
         creationTimestamp: null
         labels:
           app: {{.Values.label}}
       spec:
         containers:
         - image: {{.Values.image}}
           name: nginx
           resources: {}
   status: {}
   ```

   `vi service.yaml`

   ```
   apiVersion: v1
   kind: Service
   metadata:
     creationTimestamp: null
     labels:
       app: web1
     name: {{.Release.Name}}-svc
   spec:
     ports:
     - port: {{.Values.port}}
       protocol: TCP
       targetPort: 80
     selector:
       app: {{.Values.label}}
     type: NodePort
   status:
     loadBalancer: {}
   ```

   尝试执行

   

   ```
   [root@cluster1 k8s-learn]# helm install --dry-run web2 mychart/
   NAME: web2
   LAST DEPLOYED: Wed Dec 30 18:53:26 2020
   NAMESPACE: default
   STATUS: pending-install
   REVISION: 1
   TEST SUITE: None
   HOOKS:
   MANIFEST:
   ---
   # Source: mychart/templates/service.yaml
   apiVersion: v1
   kind: Service
   metadata:
     creationTimestamp: null
     labels:
       app: web1
     name: web2-svc
   spec:
     ports:
     - port: 80
       protocol: TCP
       targetPort: 80
     selector:
       app: nginx
     type: NodePort
   status:
     loadBalancer: {}
   ---
   # Source: mychart/templates/deployment.yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     creationTimestamp: null
     labels:
       app: web1
     name: web2-deploy
   spec:
     replicas: 1
     selector:
       matchLabels:
         app: nginx
     strategy: {}
     template:
       metadata:
         creationTimestamp: null
         labels:
           app: nginx
       spec:
         containers:
         - image: nginx
           name: nginx
           resources: {}
   status: {}
   ```

3. 完成创建

   

   ```
   [root@cluster1 k8s-learn]# helm install web2 mychart/
   NAME: web2
   LAST DEPLOYED: Wed Dec 30 18:57:52 2020
   NAMESPACE: default
   STATUS: deployed
   REVISION: 1
   TEST SUITE: None
   
   [root@cluster1 k8s-learn]# kubectl get pod,svc
   NAME                                               READY   STATUS      RESTARTS   AGE
   pod/ds-test-5dszw                                  1/1     Running     0          5d2h
   pod/ds-test-jmm4v                                  1/1     Running     0          5d2h
   pod/mypod                                          1/1     Running     0          5d1h
   pod/mypod2                                         1/1     Running     0          32h
   pod/mypod3                                         0/1     Completed   0          31h
   pod/mypod4                                         0/1     Completed   0          31h
   pod/weave-scope-agent-ws-44cq2                     1/1     Running     0          117m
   pod/weave-scope-agent-ws-9ssb5                     1/1     Running     0          117m
   pod/weave-scope-agent-ws-chksk                     1/1     Running     0          117m
   pod/weave-scope-cluster-agent-ws-99cdb686f-lhg47   1/1     Running     0          117m
   pod/weave-scope-frontend-ws-7c5f4947c-9ggl7        1/1     Running     0          117m
   pod/web2-deploy-6799fc88d8-588h5                   1/1     Running     0          6m2s
   
   NAME                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
   service/kubernetes       ClusterIP   10.10.0.1      <none>        443/TCP        6d6h
   service/web2-svc         NodePort    10.10.73.67    <none>        80:32033/TCP   6m2s
   service/ws-weave-scope   NodePort    10.10.54.125   <none>        80:31445/TCP   117m
   ```




# 持久化存储

## nfs：网络存储

1. 在一台服务器安装nfs

   `yum install -y nfs-utils`

   设置挂载路径

   `vi /etc/exports`

   ```
   /data/nfs *(rw,no_root_squash)
   ```

   `mkdir -p /data/nfs`

2. 在k8s node节点安装nfs

   `yum install -y nfs-utils`

3. 在nfs服务器启动nfs服务

   `systemctl start nfs`

4. 在k8s集群部署应用，使用nfs持久化存储

   `vi nfs-nginx.yaml`

   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: nginx-dep1
   spec:
     replicas: 1
     selector:
       matchLabels:
         app: nginx
     template:
       metadata:
         labels:
           app: nginx
       spec:
         containers:
         - name: nginx
           image: nginx
           volumeMounts:
           - name: wwwroot
             mountPath: /usr/share/nginx/html
           ports:
           - containerPort: 80
         volumes:
         - name: wwwroot
           nfs:
             server: 192.168.1.201
             path: /data/nfs
   
   
   ```

   `kubectl apply -f nfs-nginx.yaml`

5. 测试

   在nfs服务器中添加内容

   ```shell
   cd /data/nfs
   vi index.html
   ```

   ```
   hello nfs
   ```

   在pod中查看

   `kubectl exec -it nginx-dep1-6d599f4489-8vttg bash`

   ```
   [root@cluster1 k8s-learn]# kubectl exec -it nginx-dep1-6d599f4489-8vttg bash
   
   root@nginx-dep1-6d599f4489-8vttg:/# ls /usr/share/nginx/html/
   index.html
   ```

   暴露端口

   `kubectl expose deployment nginx-dep1 --port=80 --target-port=80 --type=NodePort`

   `curl localhost:`

## pv与pvc

PV：持久化存储，对存储的资源进行抽象，对外提供可以调用的地方【生产者】

PVC：用于调用，不需要关心内部实现细节【消费者】



示例：

1. 创建一个deployment和pvc

   `vi pvc.yaml`

   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: nginx-dep1
   spec:
     replicas: 3
     selector:
       matchLabels:
         app: nginx
     template:
       metadata:
         labels:
           app: nginx
       spec:
         containers:
         - name: nginx
           image: nginx
           volumeMounts:
           - name: wwwroot
             mountPath: /usr/share/nginx/html
           ports:
           - containerPort: 80
         volumes:
         - name: wwwroot
           persistentVolumeClaim:
             claimName: my-pvc
             
   ---
   
   apiVersion: v1
   kind: PersistentVolumeClaim
   metadata:
     name: my-pvc
   spec:
     accessModes:
     - ReadWriteMany
     resources:
       requests:
         storage: 5Gi
   ```

   `kubectl apply -f pvc.yaml`

   创建pv

   `vi pv.yaml`

   ```yaml
   apiVersion: v1
   kind: PersistentVolume
   metadata:
     name: my-pv
   spec:
     capacity:
       storage: 5Gi
     accessModes:
     - ReadWriteMany
     nfs:
       path: /data/nfs
       server: 192.168.1.201
   ```

   进入容器查看

   ```
   [root@cluster1 k8s-learn]# kubectl exec -it nginx-dep1-69f5bb95b-8rx8m bash
   kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
   root@nginx-dep1-69f5bb95b-8rx8m:/# ls /usr/share/nginx/html
   index.html
   root@nginx-dep1-69f5bb95b-8rx8m:/# cat /usr/share/nginx/html/index.html   
   hello nfs
   ```

   

# 集群资源监控

- 集群监控
  - 节点资源利用率
  - 节点数
  - 运行Pods
- Pod监控
  - 容器指标
  - 应用程【程序占用多少CPU、内存】

## 安装kubesphere

## 安装dashboard

下载dashboard配置文件

`wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.1.0/aio/deploy/recommended.yaml`

修改文件，添加NodePort访问方式

```
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  type: NodePort #add
  ports:
    - port: 443
      targetPort: 8443
      nodeport: 30001  #add
  selector:
    k8s-app: kubernetes-dashboard
```

安装

```
[root@cluster1 k8s-dashboard]# kubectl apply -f recommended.yaml 
namespace/kubernetes-dashboard unchanged
serviceaccount/kubernetes-dashboard unchanged
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
```

创建service account并绑定默认cluster-admin管理员角色

```shell
kubectl create serviceaccount dashboard-admin -n kube-system
kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin
```

获取token

```
kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk '/dashboard-admin/{print $1}')
```

访问`30001`端口(注意https)，输入token进入dashboard页面
