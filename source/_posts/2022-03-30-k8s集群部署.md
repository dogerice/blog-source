---
title: k8s集群部署
tags: 
  - k8s 
categories: 
  - k8s
date: 2022-03-30 00:57:47
description: k8s集群搭建过程记录
---

 





### 集群搭建

#### 环境准备

Centos7.6 三台  cluster1做主节点 另外两台做从节点

- 192.168.1.101 cluster1

- 192.168.1.102 cluster2

- 192.168.1.103 cluster3



host文件添加三台机器的映射

```
192.168.1.101 cluster1
192.168.1.102 cluster2
192.168.1.103 cluster3
```

添加阿里镜像源

```shell
mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup
curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo
```



关闭swap分区

```shell
swapoff -a
# 查看swap分区被注释掉即可
cat /etc/fstab
```

配置内核参数，将桥接的IPv4流量传递到iptables的链

```shell
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
```



#### 安装docker

```shell
yum install -y yum-utils
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum -y install docker-ce
systemctl start docker
```

配置docker镜像加速

```shell
cat > /etc/docker/daemon.json <<EOF
{
    "exec-opts": ["native.cgroupdriver=systemd"],
    "log-driver": "json-file",
    "log-opts": {
    "max-size": "100m"
    },
    "storage-driver": "overlay2",
    "registry-mirrors":[
        "https://oldepj2z.mirror.aliyuncs.com",
        "https://2lqq34jg.mirror.aliyuncs.com",
        "https://pee6w651.mirror.aliyuncs.com",
        "http://hub-mirror.c.163.com",
        "https://docker.mirrors.ustc.edu.cn",
        "https://registry.docker-cn.com"
    ]
}
EOF

# 重启 docker.
systemctl daemon-reload
systemctl restart docker
```





#### 安装kubeadm、kubelet、kubectl

在每台机器上安装以下软件包

- kubeadm:用来初始化集群的指令。
- kubelet:在集群中的每个节点上用来启动 pod 和容器等。
- kubectl:用来与集群通信的命令行工具。

添加阿里云kube镜像源

```shell
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

关闭防火墙

```shell
# 将 SELinux 设置为 permissive 模式（相当于将其禁用）
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
```

安装三个软件包，并启动kubelet

```shell
yum install -y kubelet kubeadm kubectl
systemctl enable --now kubelet
```

这时查看kubelet启动状态应该是失败的，不用管它，在`kubeadm init`前它会不断重启

#### 初始化集群

在cluster1上执行

```shell
kubeadm init --kubernetes-version=1.20.1  \
--apiserver-advertise-address=192.168.1.101   \
--image-repository registry.aliyuncs.com/google_containers  \
--service-cidr=10.10.0.0/16 --pod-network-cidr=10.122.0.0/16
```

等一段时间，返回如下信息

```shell
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.1.101:6443 --token cuv923.6fy0bwqlq07t58ac \
    --discovery-token-ca-cert-hash sha256:78a5c44670b84af07fd8d3df8d75f2ea6769abdcec73e61c3d2ad6b3bfebcd15
```

**最后两行kubeadm join 记录下来，后续加入节点时会用到**

根据返回给的提示执行：

```
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

```

#### 安装calico网络插件

```shell
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
```

装完网络插件后，查看node的状态才是Ready

```
kubectl get node 
NAME       STATUS   ROLES                  AGE   VERSION
cluster1   Ready    control-plane,master   52m   v1.20.1
```

查看pod变为Running状态

```shell
kubectl get pod --all-namespaces
NAMESPACE     NAME                                       READY   STATUS              RESTARTS   AGE
kube-system   calico-kube-controllers-744cfdf676-qgw5q   0/1     Running             0          5m
kube-system   calico-node-8ncgw                          1/1     Running             0          5m
kube-system   coredns-7f89b7bc75-6c5b4                   1/1     Running             0          54m
kube-system   coredns-7f89b7bc75-6qcxc                   1/1     Running             0          54m
kube-system   etcd-cluster1                              1/1     Running             0          54m
kube-system   kube-apiserver-cluster1                    1/1     Running             0          54m
kube-system   kube-controller-manager-cluster1           1/1     Running             0          54m
kube-system   kube-proxy-bw9lh                           1/1     Running             0          54m
kube-system   kube-scheduler-cluster1                    1/1     Running             0          54m

```

#### 加入节点

将cluster2、cluster3加入集群

在另外两台node执行命令（之前记录下来的kubeadm join命令）

```shell
kubeadm join 192.168.1.101:6443 --token cuv923.6fy0bwqlq07t58ac \
    --discovery-token-ca-cert-hash sha256:78a5c44670b84af07fd8d3df8d75f2ea6769abdcec73e61c3d2ad6b3bfebcd15
```

用命令`kubectl get node`  和 `kubectl get pod --all-namespaces` 查看节点与pod是否正常

#### 允许在Master节点部署pod

允许pod调度到master节点：

`kubectl taint nodes cluster1 node-role.kubernetes.io/master-`
如果不允许调度，则给节点打上污点
`kubectl taint nodes cluster1 node-role.kubernetes.io/master=:NoSchedule`

污点可选参数:

- NoSchedule: 一定不能被调度
- PreferNoSchedule: 尽量不要调度
- NoExecute: 不仅不会调度, 还会驱逐Node上已有的Pod



#### 测试

创建一个nginx的pod测一下

```shell
kubectl create deployment nginx --image=nginx
kubectl expose deployment nginx --port=80 --type=NodePort
```

kubectl get pod,svc -o wide 查看启动情况

```
NAME                         READY   STATUS    RESTARTS   AGE     IP               NODE       NOMINATED NODE   READINESS GATES
pod/nginx-6799fc88d8-vq2n8   1/1     Running   0          2m15s   10.122.146.193   cluster3   <none>           <none>

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE   SELECTOR
service/kubernetes   ClusterIP   10.10.0.1       <none>        443/TCP        77m   <none>
service/nginx        NodePort    10.10.226.163   <none>        80:31958/TCP   82s   app=nginx
```

nginx暴露出端口为31958，现在每台机器上都能访问31958端口看到nginx的首页了，至此证明k8s集群基本部署成功

```
$ curl localhost:31958
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```



### 故障排查

1. ` kubectl get cs ` 提示controller-manager、scheduler状态为UnHealthy

   检查kube-scheduler和kube-controller-manager组件配置是否禁用了非安全端口

   `vi /etc/kubernetes/manifests/kube-scheduler.yaml`

   `vi /etc/kubernetes/manifests/kube-controller-manager.yaml`

   将--port=0去掉

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     creationTimestamp: null
     labels:
       component: kube-controller-manager
       tier: control-plane
     name: kube-controller-manager
     namespace: kube-system
   spec:
     containers:
     - command:
       - kube-controller-manager
       - --allocate-node-cidrs=true
       - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
       - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
       - --bind-address=127.0.0.1
       - --client-ca-file=/etc/kubernetes/pki/ca.crt
       - --cluster-cidr=10.122.0.0/16
       - --cluster-name=kubernetes
       - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
       - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
       - --controllers=*,bootstrapsigner,tokencleaner
       - --kubeconfig=/etc/kubernetes/controller-manager.conf
       - --leader-elect=true
   #    - --port=0 #注释掉这行
       - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
       - --root-ca-file=/etc/kubernetes/pki/ca.crt
       - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
       - --service-cluster-ip-range=10.10.0.0/16
       - --use-service-account-credentials=true
   ```

   修改完稍等会自动变为Healthy状态，或`systemctl restart kubelet`重启

2. 在node节点无法正常使用kubectl 命令

   node节点使用kubectl命令时出现如下错误

   ```shell
   [root@cluster2 ~]# kubectl get pod
   The connection to the server localhost:8080 was refused - did you specify the right host or port?
   ```

   原因是kubectl命令需要使用kubernetes-admin来运行，解决方法如下

   将master节点下`/etc/kubernetes/admin.conf`文件拷贝到从node节点相同目录下,并配置环境变量：

   ```shell
   echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> ~/.bash_profile
   source ~/.bash_profile
   ```

   

### 卸载集群/节点

在需要卸载的节点上执行`kubeadm reset` 命令，并根据提示手动删除文件`$HOME/.kube/config`  ,卸载后的节点可以重新`kubeadm join`加入集群



### 高可用k8s集群(缺服务器，未实操)

以上的方式部署的为1主2从的集群，单主节点显然不能够高可用

部署高可用集群至少需要3台Master节点和3台Worker节点，并保证数量都为奇数，防止leader选举出现脑裂

在上面集群的基础上进行扩展：

在master上生成新的token

```shell
[root@cluster1 ~]# kubeadm token create --print-join-command
kubeadm join 192.168.1.101:6443 --token ortvag.ra0654faci8y8903     --discovery-token-ca-cert-hash sha256:04755ff1aa88e7db283c85589bee31fabb7d32186612778e53a536a297fc9010
```

- 添加新node

  用上面生成的命令在节点上执行

- 添加新master

  在原主节点上生成证书

  ```shell
  kubeadm init phase upload-certs --experimental-upload-certs
  ```

  得到证书内容：

  ```shell
  [upload-certs] Storing the certificates in ConfigMap "kubeadm-certs" in the "kube-system" Namespace
  [upload-certs] Using certificate key:
  f8d1c027c01baef6985ddf24266641b7c64f9fd922b15a32fce40b6b4b21e47d
  ```

  在新的主节点执行

  ```shell
  kubeadm join 172.31.182.156:8443  --token ortvag.ra0654faci8y8903 \
    --discovery-token-ca-cert-hash sha256:04755ff1aa88e7db283c85589bee31fabb7d32186612778e53a536a297fc9010 \
    --experimental-control-plane --certificate-key f8d1c027c01baef6985ddf24266641b7c64f9fd922b15a32fce40b6b4b21e47d
  ```

  





